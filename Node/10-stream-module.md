
---

# ğŸ“¦ Node.js `stream` Module â€” COMPLETE NOTES 

---

## 1ï¸âƒ£ What is a Stream? (Very Simple)

### ğŸ“Œ Definition

A **stream** is a way to **read or write data piece by piece (chunks)** instead of loading everything at once.

### ğŸ§  Real-life example

Imagine watching a YouTube video:

âŒ Bad way (no stream)  
â†’ Download whole video first  
â†’ Then play

âœ… Good way (stream)  
â†’ Download small chunks  
â†’ Play while downloading

ğŸ‘‰ Node.js streams work like this.

---

## 2ï¸âƒ£ Why Streams are IMPORTANT?

### âŒ Without streams

```js
fs.readFile("bigfile.mp4", (err, data) => {
  // loads entire file into memory
});
```

Problems:

- High memory usage
    
- Slow
    
- App can crash for large files
    

### âœ… With streams

```js
fs.createReadStream("bigfile.mp4");
```

Benefits:

- Low memory usage
    
- Faster
    
- Efficient
    
- Scales well
    

---

## 3ï¸âƒ£ Types of Streams in Node.js

Node.js has **4 types of streams** ğŸ‘‡

```txt
1. Readable  â†’ Read data
2. Writable  â†’ Write data
3. Duplex    â†’ Read + Write
4. Transform â†’ Modify data while reading/writing
```

---

## 4ï¸âƒ£ Readable Stream (MOST IMPORTANT)

### ğŸ“Œ What is Readable Stream?

Used to **read data chunk by chunk**.

### Examples:

- Reading a file
    
- Incoming HTTP request
    
- Reading data from DB
    

---

### ğŸ”¹ Create Readable Stream (File)

```js
const fs = require("fs");

const readStream = fs.createReadStream("file.txt");
```

---

### ğŸ”¹ Important Events of Readable Stream

#### 1ï¸âƒ£ `data` event

Fires when a chunk is available.

```js
readStream.on("data", (chunk) => {
  console.log(chunk);
});
```

---

#### 2ï¸âƒ£ `end` event

Fires when reading is finished.

```js
readStream.on("end", () => {
  console.log("File reading finished");
});
```

---

#### 3ï¸âƒ£ `error` event

Fires if error occurs.

```js
readStream.on("error", (err) => {
  console.log(err);
});
```

---

### ğŸ”¹ Complete Readable Example

```js
const fs = require("fs");

const readStream = fs.createReadStream("file.txt", "utf8");

readStream.on("data", (chunk) => {
  console.log("Chunk:", chunk);
});

readStream.on("end", () => {
  console.log("Reading completed");
});
```

### ğŸ“¤ Output (example)

```sh
Chunk: Hello
Chunk: World
Reading completed
```

---

## 5ï¸âƒ£ Writable Stream

### ğŸ“Œ What is Writable Stream?

Used to **write data chunk by chunk**.

### Examples:

- Writing to a file
    
- Sending HTTP response
    

---

### ğŸ”¹ Create Writable Stream

`const fs = require("fs");  const writeStream = fs.createWriteStream("output.txt");`

---

### ğŸ”¹ Writing Data

`writeStream.write("Hello "); writeStream.write("World");`

---

### ğŸ”¹ Ending the Stream

`writeStream.end();`

---

### ğŸ”¹ Events in Writable Stream

#### 1ï¸âƒ£ `finish`

Triggered when writing is done.

`writeStream.on("finish", () => {   console.log("Writing finished"); });`

---

### ğŸ”¹ Complete Writable Example

`const fs = require("fs");  const writeStream = fs.createWriteStream("output.txt");  writeStream.write("Node "); writeStream.write("Streams");  writeStream.end();  writeStream.on("finish", () => {   console.log("Data written successfully"); });`

---

## 6ï¸âƒ£ pipe() â€” MOST IMPORTANT METHOD â­â­â­â­â­

### ğŸ“Œ What is `pipe()`?

`pipe()` connects **Readable â†’ Writable** automatically.

### âŒ Without pipe

You manually handle chunks.

### âœ… With pipe

Node handles everything.

---

### ğŸ”¹ Syntax

`readableStream.pipe(writableStream);`

---

### ğŸ”¹ File Copy Example (BEST EXAMPLE)

`const fs = require("fs");  const readStream = fs.createReadStream("source.txt"); const writeStream = fs.createWriteStream("dest.txt");  readStream.pipe(writeStream);`

âœ” Efficient  
âœ” Fast  
âœ” Low memory

---

## 7ï¸âƒ£ Duplex Stream

### ğŸ“Œ What is Duplex Stream?

Can **read and write both**.

### Examples:

- TCP socket
    
- WebSocket
    

---

### ğŸ”¹ Example (basic concept)

`const { Duplex } = require("stream");  const duplex = new Duplex({   read(size) {     this.push("Hello");     this.push(null);   },   write(chunk, encoding, callback) {     console.log(chunk.toString());     callback();   } });  duplex.on("data", data => console.log(data.toString())); duplex.write("World");`

---

## 8ï¸âƒ£ Transform Stream (VERY IMPORTANT)

### ğŸ“Œ What is Transform Stream?

A special Duplex stream that **modifies data**.

### Examples:

- Compression
    
- Encryption
    
- Uppercase / lowercase
    
- JSON parsing
    

---

### ğŸ”¹ Example: Convert to Uppercase

`const { Transform } = require("stream");  const upperCaseTransform = new Transform({   transform(chunk, encoding, callback) {     this.push(chunk.toString().toUpperCase());     callback();   } });  process.stdin.pipe(upperCaseTransform).pipe(process.stdout);`

---

### ğŸ§  Flow

`Input â†’ Transform â†’ Output`

---

## 9ï¸âƒ£ Backpressure (IMPORTANT CONCEPT)

### ğŸ“Œ What is Backpressure?

When **Writable stream is slow** and **Readable stream is fast**.

### Example:

- Fast file read
    
- Slow network write
    

Node handles this automatically when using `pipe()`.

ğŸ‘‰ **This is why pipe() is recommended**

---

## ğŸ”Ÿ Stream States

Readable stream states:

- flowing
    
- paused
    
- ended
    

Writable stream states:

- writing
    
- finished
    

---

## 1ï¸âƒ£1ï¸âƒ£ Stream vs Buffer vs fs.readFile

|Feature|fs.readFile|Buffer|Stream|
|---|---|---|---|
|Memory|High|Medium|Low|
|Speed|Slow|Medium|Fast|
|Large files|âŒ|âŒ|âœ…|

---

## 1ï¸âƒ£2ï¸âƒ£ Streams in HTTP (IMPORTANT)

### Sending file using stream

`const http = require("http"); const fs = require("fs");  http.createServer((req, res) => {   const stream = fs.createReadStream("file.txt");   stream.pipe(res); }).listen(3000);`

âœ” Fast  
âœ” Efficient  
âœ” Production ready

---

## 1ï¸âƒ£3ï¸âƒ£ Important Stream Methods Summary

### Readable

- `.on("data")`
    
- `.on("end")`
    
- `.pipe()`
    
- `.resume()`
    
- `.pause()`
    

### Writable

- `.write()`
    
- `.end()`
    
- `.on("finish")`
    

### Transform

- `transform(chunk, enc, cb)`
    

---

## ğŸ§  One-Line Memory Tricks

- **Stream** â†’ Data in chunks
    
- **pipe()** â†’ Best way to handle streams
    
- **Readable** â†’ Read data
    
- **Writable** â†’ Write data
    
- **Transform** â†’ Change data
    
- **Backpressure** â†’ Auto-handled by pipe()
    

---

## ğŸ¯ Interview One-Liners

- Streams handle large data efficiently by processing chunks instead of loading whole data into memory.
    
- `pipe()` manages backpressure automatically.
    
- Transform streams modify data during streaming.